{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P3 - Analyse et prédiction de données Covid-19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Léon Muller - Projet P3 - 2021/2022 - Haute Ecole Arc - Projet n°221\n",
    "\n",
    "Superviseur: M. Ninoslav"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sommaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Cahier des charges\n",
    "\t1. Récupération des données\n",
    "\t2. Visualisation des données\n",
    "\t3. Analyse des données et prédictions\n",
    "2. Résumé\n",
    "3. Introduction\n",
    "\t1. Récupération de données\n",
    "\t2. Visualisation des données\n",
    "\t3. Analyse de données et prédictions\n",
    "\t4. Bibliothèques utilisées\n",
    "4. État de l'art\n",
    "5. Planing\n",
    "6. Récupération des données\n",
    "7. Prétraitement\n",
    "\t1. Suppression des colonnes redondantes\n",
    "\t2. Analyse des NaN (Not a Number)\n",
    "\t3. Analyse de forme\n",
    "\t4. Analyse des corrélations\n",
    "\t5. Mise en forme\n",
    "8. Prédictions\n",
    "\t1. Définitions\n",
    "\t\t1. Types d'algorithmes\n",
    "\t\t2. Types de problèmes\n",
    "\t\t3. Caractéristique d'un modèle\n",
    "\t2. Régression avec données\n",
    "\t\t1. Naive Bayes Classifier\n",
    "\t\t\t1. Gaussian\n",
    "\t\t\t1. Multinomial\n",
    "\t\t2. K-Nearest Neighbors\n",
    "\t\t3. Decision Trees\n",
    "\t\t\t1. Simple Decision Trees\n",
    "\t\t\t2. Random Forest\n",
    "\t\t\t3. Gradient Boosted Trees\n",
    "\t\t4. Support Vector Regression\n",
    "\t\t5. Multi-Layer Perceptrons (Deep Learning)\n",
    "\t3. Régression sans données\n",
    "9. Conclusion\n",
    "10. Bibliographie\n",
    "\t1. Sources documentaires\n",
    "\t2. Sources illustratives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Cahier des charges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'intitulé du projet P3 est \"Analysis and prediction of Covid-19 data using Python\".\n",
    "Ce projet a pour but de récolter des données sur le Covid (nombre de morts, de vaccinés, de lits de réanimation libres, etc ...) afin de les affichés de manière intuitive, compréhensive et utile. Pour ce faire, il sera nécessaire de former avec ces différentes données des schémas, diagrammes, cartes interactives, etc... pour permettre une visualisation la plus claire et pertinente possible. De plus, il faudra analyser ces différentes données afin de pouvoir en prédire les évolutions à venir.\n",
    "\n",
    "Ce projet se découpe donc en trois grandes parties : récupération d'un set de données, visualisation des données récupérées, ainsi que prédictions des futurs évènements.\n",
    "\n",
    "## 1.1 Récupération des données\n",
    "\n",
    "- Recherche et choix de bases de données viables pour entrainer le modèle de machine learning, et libres d'accès\n",
    "- Choix des données à traiter\n",
    "- Mise en place d'un parseur pour traiter les données récoltées\n",
    "\n",
    "Ex: our world in data\n",
    "\n",
    "## 1.2 Visualisation des données\n",
    "\n",
    "- Chercher, choisir et représenter les données sous la forme la plus adaptée et compréhensible possible (graphiques, diagrammes, ...)\n",
    "- Choisir les différents paramètres pris en compte pour l'affichage des données (catégorie, temporalité, ...)\n",
    "- Prise en mains des outils de visualisation tels que Pandas ou Seaborn\n",
    "\n",
    "Voici quelques idées de données pertinentes à afficher :\n",
    "\n",
    "- Couverture vaccinale (1e et 2nd dose)\n",
    "- Nombre de tests positifs/négatifs (vaccinés ou non)\n",
    "- Nombre d'hospitalisations\n",
    "- Nombre d'admis en réanimation (vaccinés ou non)\n",
    "- Nombre de nouveaux cas pour les mutations\n",
    "- Taux de reproduction effectif (nombre de personnes contaminées par personne positive)\n",
    "- Tension des reanimation\n",
    "- Positifs, morts etc en cummulés\n",
    "- Centres de dépistage sur une carte avec la position actuelle de l'ordinateur\n",
    "- Carte des nombre de cas, lit de rea dispo, etc\n",
    "\n",
    "Ce projet peut s'inspirer de sites internet ou applications déjà existantes permettant la visualisation de données liées au Covid tels que : https://www.gouvernement.fr/info-coronavirus/carte-et-donnees\n",
    "\n",
    "Voici aussi quelques exemples de façons de représenter les données récoltées :\n",
    "\n",
    "<img src=\"https://i.imgur.com/mGcibKB.png\" height=\"200\"/>\n",
    "<img src=\"https://i.imgur.com/qUUrw4Q.png\" height=\"200\"/>\n",
    "<img src=\"https://i.imgur.com/dO0Ysa3.png\" height=\"200\"/>\n",
    "<img src=\"https://i.imgur.com/9sQqpWY.png\" height=\"200\"/>\n",
    "<img src=\"https://i.imgur.com/VSm6qHf.png\" height=\"200\"/>\n",
    "\n",
    "## 1.3 Analyse des données et prédictions\n",
    "\n",
    "- Recherche et choix du/des modèle(s) utilisé(s) pour la prédiction des données\n",
    "- Prise en mains des outils de machine learning tels que Tensorflow ou Keras\n",
    "- Comparaison de différents modèles de machine learning afin d'en déterminer le/les meilleur(s)\n",
    "- Test du/des modèle(s) de machine learning sélectionné(s) et réflexion sur leur efficacité\n",
    "- Affichage des données prédites en rapport avec les données récoltées au préalable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Résumé"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Version française"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce projet a pour sujet \"Analysis and prediction of Covid-19 data using Python\". Le projet a pour but d'utiliser une base de données libre d'accès et mise à jour quotidiennement afin d'en extraire des données utiles, et ainsi tenter de prédire certaines données avant qu'elles ne soient révélées. Pour ce projet a été choisi comme donnée de prédiction, le nombre de mort du Covid chaque jour. L'objectif est donc de prédire cette information grâce aux autres données disponibles dans la base de données. \n",
    "\n",
    "Dans un premier temps, une base de données a été choisie: Our world in data. Cette base de données recense chaque jour des milliers d'informations liées au Covid-19 dans près de 250 pays du monde. Il est ensuite nécessaire de filtrer les données récoltées pour n'en garder que les plus pertinentes. Pour ce faire, différentes analyses ont été réalisées permettant de supprimer les données où le nombre d'informations récoltées est trop faible, mais aussi de ne garder que les données ayant un lien avec le nombre de morts par jour, ou encore les données redondantes telles que le nombre de cas positif au Covid par milliers, par millions, etc. De plus, dans le cadre de ce projet, je ne me suis uniquement concentré que sur les données provenant de la Suisse.\n",
    "\n",
    "Une fois les données filtrées et analysées, elles ont été utilisées afin d'entrainer des algorithmes d'apprentissage automatique. J'ai entrainé plusieurs algorithmes pour pouvoir les comparer, comprendre leur fonctionnement, les optimiser et ainsi trouver celui ayant les meilleures performances, permettant donc de prédire au mieux le nombre de morts du Covid chaque jour. Les différentes prédictions et données récoltées pour chaque jour ont été représentées sur des graphiques permettant visuellement de comprendre les résultats obtenus. Il est ainsi possible de voir comment se comporte chaque algorithme, et en faire des déductions.\n",
    "\n",
    "Cependant, cette méthode ne permet que de prédire par exemple le nombre de morts du jour actuel en utilisant les données de ce jour. J'ai donc tenté d'aller plus loin, et de prédire le nombre de morts pour la ou les semaines à venir, mais évidemment sans données. Cette tentative ne s'est pas avérée concluante, mais elle soulève des questions et amène à des améliorations possibles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Version anglaise "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project is about \"Analysis and prediction of Covid-19 data using Python\". The aim of the project is to use an open source database that is updated daily to extract useful data, and thus attempt to predict certain data before they are revealed. For this project, the number of Covid deaths per day was chosen as the predictive data. The objective is to predict this information using the other data available in the database. \n",
    "\n",
    "In a first step, a database was chosen: Our world in data. This database lists thousands of pieces of information related to Covid-19 in nearly 250 countries around the world every day. It is then necessary to filter the collected data to keep only the most relevant. To do this, various analyses were carried out to remove data where the number of information collected is too low, but also to keep only the data related to the number of deaths per day, or redundant data such as the number of Covid-positive cases per thousand, per million, etc. In addition, for this project, I only focused on data from Switzerland.\n",
    "\n",
    "Once the data was filtered and analysed, it was used to train machine learning algorithms. I trained several algorithms to compare them, understand how they work, optimise them and find the one with the best performance, thus allowing me to predict the number of Covid deaths each day. The different predictions and data collected for each day were represented on graphs allowing a visual understanding of the results obtained. It is thus possible to see how each algorithm behaves, and to make inferences.\n",
    "\n",
    "However, this method can only predict, for example, the number of deaths on the current day using the data for that day. So I tried to go further, and predict the number of deaths for the next week or two, but obviously without data. This attempt was not conclusive, but it raises questions and leads to possible improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depuis le début de la crise sanitaire de Covid-19 en 2020, nous avons connu un grand changement dans nos vies quotidiennes. Les routines ont changé, ainsi que notre de travailler, ou encore d'interagir et même de nous comporter. Chaque jour, un grand nombre de données relatives au Covid-19 sont collectées sur le nombre de patients infectés, testés, décédés, guéris ou vaccinés. Les données sur les différentes mutations et les différents vaccins dans tous les pays du monde sont soigneusement collectées et pas toujours analysées ou visualisées de manière à pouvoir tirer certaines conclusions.\n",
    "\n",
    "TODO agrandir l'introduction\n",
    "\n",
    "Dans ce projet, l'objectif est donc de collecter et analyser les données Covid-19 pour différents pays et régions. Nous ferons des analyses statistiques et proposerons différentes méthodes de visualisation. En outre, nous essaierons de faire des prédictions en utilisant certains des outils et modules fournis par Python.\n",
    "\n",
    "Ce projet se découpe donc en trois grandes parties : récupération de données, visualisation des données récupérées, ainsi que prédictions des futurs évènements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Récupération de données\n",
    "\n",
    "Au début du projet, il faudra chercher et choisir une base de données viable et libre d'accès pour entrainer le modèle de machine learning. Il faudra ensuite choisir les données à traiter, ainsi que mettre en place une manière de traiter les données récoltées."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Visualisation des données\n",
    "\n",
    "Par la suite, il faudra chercher, choisir et représenter les données sous la forme la plus adaptée et compréhensible possible (graphiques, diagrammes ...). Par la suite, il sera nécessaire de choisir les différents paramètres pris en compte pour l'affichage des données (catégorie, temporalité ...). \n",
    "Pour cela, il faudra au préalable prendre en main des outils de visualisation tels que Pandas ou Seaborn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Analyse de données et prédictions\n",
    "\n",
    "Enfin, il faudra chercher, comparer, choisir et tester le/les modèle(s) de machine learning utilisé(s) permettant la prédiction du nombre de nouveaux morts chaque jour. Pour cela, il faudra au préalable prendre en main des outils de machine learning tels que Scikit-learn. Pour finir, le nombre de nouveaux morts chaque jour devra être affiché en rapport aux données récoltées au préalable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Bibliothèques utilisées"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisation: \n",
    "- Matplotlib: Diagrammes simples et compréhensifs\n",
    "- Seaborn: Diagrammes complexes et complets\n",
    "- Plotly: Diagrammes interactifs\n",
    "\n",
    "Représentation:\n",
    "- Numpy: Représentation simpliste des données\n",
    "- Pandas: Représentation des données sous forme de tableau labélisé\n",
    "\n",
    "Machine learning:\n",
    "- Scikit-learn: Modèles de machine learning, évaluation des modèles, mise en forme des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.dont_write_bytecode = True # Stop creating __pycache__ folder\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "from toolbox import * # My own toolbox\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. État de l'art"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "\n",
    "État de l'art\n",
    "\n",
    "Domaines dans lesquels on pourrait utiliser aussi ce projet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Planing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Au début du projet, un planing a été réalisé afin de diviser le projet en plusieurs tâches, et ainsi permettant de voir l'avance ou le retard pris régulièrement en fonction des objectifs fixés.\n",
    "\n",
    "<img src=\"https://i.imgur.com/J7oYvJ3.png\" height=\"500\">\n",
    "\n",
    "Le planing a été en majorité respecté. Cependant, les étapes de visualisation des données ont été réalisées en parallèle des étapes de prédiction et de prise en main des modèles de machine learning. Il était nécessaire de pouvoir constater visuellement les données résultant des prédictions des différents modèles. De plus, le choix de base de données et donc la partie traitement des données a été bien plus longue que prévue initialement.\n",
    "\n",
    "Les écarts réalisés ont pu tout de même être rattrapés avec une charge de travail plus importantes durant certaines semaines. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Récupération des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our World In Data est un site internet recensant un nombre impressionnant de données sur tout type de sujets à travers le monde. Les publications présentent sur le site sont dirigées par l'université d'Oxford et rédigées par l'historien social et économiste du développement Max Roser.\n",
    "\n",
    "Ce site internet met à disposition une base de données accessible via GitHub: https://github.com/owid/covid-19-data/tree/master/public/data, ou directement sur leur site internet: https://ourworldindata.org/coronavirus.\n",
    "\n",
    "Il y a à disposition plusieurs formats afin de récupérer cette base de données: csv, xlsx ou json. Json étant un format plus simple de compréhension, c'est celui que j'ai choisi de traiter. Cette base de données est tenue à jour et est actualisée chaque jour, ainsi les données traitées dans ce projet seront toujours actualisées.\n",
    "\n",
    "Grâce à la bibliothèque Pandas, on peut récupérer les données en passant l'URL de recherche à la méthode `read_json`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://covid.ourworldindata.org/data/owid-covid-data.json'\n",
    "json = pd.read_json(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois les données récoltées, il va falloir les traiter afin d'en extraire les parties importantes et utiles. \n",
    "\n",
    "Dans le cadre de ce projet, nous allons traiter les données relatives à la Suisse, en utilisant donc l'ISO `CHE`. Les données Covid de chaque jour se trouvent ensuite dans la partie `data` de la base de données. \n",
    "\n",
    "On peut voir avec `tail`, que les données sont bien actualisées chaque jour. On peut aussi voir la forme des données, ainsi que les différentes informations contenues dans cette base de données. En effet, 45 données sont récoltées chaque jour en Suisse, ce nombre peut être différent en fonction du pays que l'on souhaite traiter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>total_cases</th>\n",
       "      <th>new_cases</th>\n",
       "      <th>total_cases_per_million</th>\n",
       "      <th>new_cases_per_million</th>\n",
       "      <th>stringency_index</th>\n",
       "      <th>new_cases_smoothed</th>\n",
       "      <th>new_cases_smoothed_per_million</th>\n",
       "      <th>weekly_hosp_admissions</th>\n",
       "      <th>weekly_hosp_admissions_per_million</th>\n",
       "      <th>...</th>\n",
       "      <th>total_vaccinations_per_hundred</th>\n",
       "      <th>people_vaccinated_per_hundred</th>\n",
       "      <th>people_fully_vaccinated_per_hundred</th>\n",
       "      <th>new_vaccinations</th>\n",
       "      <th>new_vaccinations_smoothed</th>\n",
       "      <th>new_vaccinations_smoothed_per_million</th>\n",
       "      <th>new_people_vaccinated_smoothed</th>\n",
       "      <th>new_people_vaccinated_smoothed_per_hundred</th>\n",
       "      <th>total_boosters</th>\n",
       "      <th>total_boosters_per_hundred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>2022-01-14</td>\n",
       "      <td>1666441.0</td>\n",
       "      <td>32183.0</td>\n",
       "      <td>191204.423</td>\n",
       "      <td>3692.619</td>\n",
       "      <td>44.44</td>\n",
       "      <td>26175.571</td>\n",
       "      <td>3003.338</td>\n",
       "      <td>510.0</td>\n",
       "      <td>58.516</td>\n",
       "      <td>...</td>\n",
       "      <td>168.97</td>\n",
       "      <td>69.11</td>\n",
       "      <td>67.49</td>\n",
       "      <td>77459.0</td>\n",
       "      <td>64470.0</td>\n",
       "      <td>7397.0</td>\n",
       "      <td>4055.0</td>\n",
       "      <td>0.047</td>\n",
       "      <td>2933890.0</td>\n",
       "      <td>33.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>690</th>\n",
       "      <td>2022-01-15</td>\n",
       "      <td>1666441.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>191204.423</td>\n",
       "      <td>0.000</td>\n",
       "      <td>44.44</td>\n",
       "      <td>26175.571</td>\n",
       "      <td>3003.338</td>\n",
       "      <td>493.0</td>\n",
       "      <td>56.566</td>\n",
       "      <td>...</td>\n",
       "      <td>169.45</td>\n",
       "      <td>69.18</td>\n",
       "      <td>67.52</td>\n",
       "      <td>42240.0</td>\n",
       "      <td>63351.0</td>\n",
       "      <td>7269.0</td>\n",
       "      <td>4201.0</td>\n",
       "      <td>0.048</td>\n",
       "      <td>2966011.0</td>\n",
       "      <td>34.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>691</th>\n",
       "      <td>2022-01-16</td>\n",
       "      <td>1666441.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>191204.423</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26175.571</td>\n",
       "      <td>3003.338</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>169.60</td>\n",
       "      <td>69.21</td>\n",
       "      <td>67.53</td>\n",
       "      <td>12929.0</td>\n",
       "      <td>61859.0</td>\n",
       "      <td>7098.0</td>\n",
       "      <td>4254.0</td>\n",
       "      <td>0.049</td>\n",
       "      <td>2975580.0</td>\n",
       "      <td>34.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>692</th>\n",
       "      <td>2022-01-17</td>\n",
       "      <td>1734346.0</td>\n",
       "      <td>67905.0</td>\n",
       "      <td>198995.720</td>\n",
       "      <td>7791.297</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26781.286</td>\n",
       "      <td>3072.836</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>170.22</td>\n",
       "      <td>69.25</td>\n",
       "      <td>67.57</td>\n",
       "      <td>54494.0</td>\n",
       "      <td>59454.0</td>\n",
       "      <td>6822.0</td>\n",
       "      <td>4173.0</td>\n",
       "      <td>0.048</td>\n",
       "      <td>3022316.0</td>\n",
       "      <td>34.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693</th>\n",
       "      <td>2022-01-18</td>\n",
       "      <td>1763497.0</td>\n",
       "      <td>29151.0</td>\n",
       "      <td>202340.453</td>\n",
       "      <td>3344.733</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27430.857</td>\n",
       "      <td>3147.367</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  total_cases  new_cases  total_cases_per_million  \\\n",
       "689  2022-01-14    1666441.0    32183.0               191204.423   \n",
       "690  2022-01-15    1666441.0        0.0               191204.423   \n",
       "691  2022-01-16    1666441.0        0.0               191204.423   \n",
       "692  2022-01-17    1734346.0    67905.0               198995.720   \n",
       "693  2022-01-18    1763497.0    29151.0               202340.453   \n",
       "\n",
       "     new_cases_per_million  stringency_index  new_cases_smoothed  \\\n",
       "689               3692.619             44.44           26175.571   \n",
       "690                  0.000             44.44           26175.571   \n",
       "691                  0.000               NaN           26175.571   \n",
       "692               7791.297               NaN           26781.286   \n",
       "693               3344.733               NaN           27430.857   \n",
       "\n",
       "     new_cases_smoothed_per_million  weekly_hosp_admissions  \\\n",
       "689                        3003.338                   510.0   \n",
       "690                        3003.338                   493.0   \n",
       "691                        3003.338                     NaN   \n",
       "692                        3072.836                     NaN   \n",
       "693                        3147.367                     NaN   \n",
       "\n",
       "     weekly_hosp_admissions_per_million  ...  total_vaccinations_per_hundred  \\\n",
       "689                              58.516  ...                          168.97   \n",
       "690                              56.566  ...                          169.45   \n",
       "691                                 NaN  ...                          169.60   \n",
       "692                                 NaN  ...                          170.22   \n",
       "693                                 NaN  ...                             NaN   \n",
       "\n",
       "     people_vaccinated_per_hundred  people_fully_vaccinated_per_hundred  \\\n",
       "689                          69.11                                67.49   \n",
       "690                          69.18                                67.52   \n",
       "691                          69.21                                67.53   \n",
       "692                          69.25                                67.57   \n",
       "693                            NaN                                  NaN   \n",
       "\n",
       "     new_vaccinations  new_vaccinations_smoothed  \\\n",
       "689           77459.0                    64470.0   \n",
       "690           42240.0                    63351.0   \n",
       "691           12929.0                    61859.0   \n",
       "692           54494.0                    59454.0   \n",
       "693               NaN                        NaN   \n",
       "\n",
       "     new_vaccinations_smoothed_per_million  new_people_vaccinated_smoothed  \\\n",
       "689                                 7397.0                          4055.0   \n",
       "690                                 7269.0                          4201.0   \n",
       "691                                 7098.0                          4254.0   \n",
       "692                                 6822.0                          4173.0   \n",
       "693                                    NaN                             NaN   \n",
       "\n",
       "     new_people_vaccinated_smoothed_per_hundred  total_boosters  \\\n",
       "689                                       0.047       2933890.0   \n",
       "690                                       0.048       2966011.0   \n",
       "691                                       0.049       2975580.0   \n",
       "692                                       0.048       3022316.0   \n",
       "693                                         NaN             NaN   \n",
       "\n",
       "     total_boosters_per_hundred  \n",
       "689                       33.66  \n",
       "690                       34.03  \n",
       "691                       34.14  \n",
       "692                       34.68  \n",
       "693                         NaN  \n",
       "\n",
       "[5 rows x 47 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = json['CHE']['data']\n",
    "df = pd.DataFrame(data)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette base de données, se trouvent aussi quelques informations à propos de chaque pays: le continent, la population, l'âge médian, le nombre de lits d'hôpitaux ... Toutes ces informations ne seront pas utilisées dans le cadre de ce projet, mais peuvent avoir une importance lors de la comparaison entre les données de différents pays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in json['CHE'].items(): \n",
    "\tif key != 'data':\n",
    "\t\tprint(f\"{key}: {value}\")\n",
    "\n",
    "print(f\"\\n\\nData gathered on {len(df)} days, from {df['date'][0]} to {df['date'][len(df) - 1]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Prétraitement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dues à la grande taille de la base de données, ces données brutes sont généralement de faible qualité. Elles peuvent être incomplètes (valeurs manquantes), bruitées (valeurs erronées ou aberrantes) ou incohérentes (divergence entre attributs). Il est donc nécessaire d'effectuer un prétraitement sur ces données, d’améliorer la qualité des données.\n",
    "\n",
    "Dans cette partie, on va donc modifier, travailler les données à disposition grâce à Pandas, afin de supprimer les parties inutilisées, d'uniformiser les données ... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Étant donné que l'intégralité du projet se trouve dans ce notebook, il est vivement recommandé de ne jamais travailler directement sur le dataframe collecté, mais de passer par des copies pour pouvoir à tout moment récupérer les données brutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_proc = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Suppression des colonnes redondantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans la base de données se trouvent des colonnes redondantes, colonnes n'apportant aucune nouvelle information telles que le nombre de nouveaux cas par millions d'habitants, le nombre de cas lissé, ... Toutes ces colonnes ne seront pas utiles pour entrainer les modèles de machine learning, il faut donc les supprimer. ²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_proc = df_proc.drop(list(df_proc.filter(regex='smoothed|million|hundred|thousand')), axis=1)\n",
    "print(df_proc.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Analyse des NaN (Not a Number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans notre base de données se trouvent des colonnes qui sont peu remplies (case remplie avec NaN), en effet les données collectées sur ces sujets n'ont pas été récupérées tous les jours, ou n'ont pas été collectées depuis le début de l'épidémie.\n",
    "\n",
    "En effet, on peut voir sur le graphique ci-dessous le taux de données non répertoriées (en beige) pour chacune des colonnes de la base de données. Certaines données ne sont pas récoltées chaque jour (ex: weekly hosp admissions), et d'autres n'ont commencé à être récoltées qu'un certain temps après le début de la pandémie (ex: new vaccinations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "sns.heatmap(df_proc.isna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici aussi, sous forme textuelle, le schéma présent ci-dessus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trie de différentes données pour voir lesquelles possèdent le plus de NaN\n",
    "missing_rate = df_proc.isna().sum() / df_proc.shape[0]\n",
    "print(\"Missing rate:\")\n",
    "print(round(missing_rate.sort_values(ascending=False), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les données étant collectées que depuis moins de 2 ans, la base de données mise à disposition est donc de faible taille. Il a donc fallu garder un maximum de colonnes, sans pour autant fausser les résultats pouvant se baser sur une partie trop importante de données manquantes. J'ai donc choisi de ne travailler que sur les colonnes contenant au moins 50% de données depuis le début de l'épidémie. Voici donc les différentes données restantes récoltées chaque jour disponibles après ce deuxième traitement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression des colonnes qui ont moins de 50% des données\n",
    "df_proc = df_proc.drop(df_proc.columns[missing_rate > 0.5], axis=1)\n",
    "print(df_proc.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Analyse de forme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Étant donné que le but du projet est de prédire le nombre de nouveaux morts par jour, nous allons travailler sur des modèles de machine learning basés sur la régression: prédire un nombre le plus proche de la réalité possible. Il est donc nécessaire de travailler sur des données sous formes numériques, ainsi vérifions le type de données présentes.\n",
    "\n",
    "On doit tout de même garder la colonne `date` sous forme non numérique, car cette colonne deviendra par la suite l'indice du tableau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_proc.select_dtypes('object'):\n",
    "\tif col != 'date':\n",
    "\t\tdf_proc[col] = df_proc[col].fillna('-')\n",
    "\t\tplt.figure()\n",
    "\t\tdf_proc[col].value_counts().plot.pie()\n",
    "\t\tprint(f'{col :-<30} {df_proc[col].unique()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La colonne `tests_units` n'est pas sous forme numérique, il faut donc la supprimer de nos données. Pour ce faire, on supprime donc toutes les colonnes qui sont sous la forme 'object', cependant cette méthode supprime aussi la colonne `date`, il faudra donc la rajouter par la suite, mais pour l'instant la supprimer aussi n'a pas d'importance. Voici donc les différentes données restantes récoltées chaque jour disponibles après ce troisième traitement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_proc = df_proc.select_dtypes(exclude=['object'])\n",
    "print(df_proc.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 Analyse des corrélations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin de supprimer les colonnes ne permettant pas d'aider le modèle de machine learning à mieux prédire le nombre de morts chaque jour, il est nécessaire d'effectuer une analyse de corrélation entre les différentes informations contenues dans la base de données. La corrélation mesure une dépendance linéaire entre deux variables. L'analyse de corrélation permet donc d’étudier la dépendance entre le nombre de morts chaque jour et les autres informations à disposition non supprimées jusqu'ici.\n",
    "\n",
    "On peut représenter ces corrélations grâce à une matrice de corrélation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.clustermap(df_proc.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est seulement nécessaire de se focaliser sur les corrélations en rapport avec ce qui sera prédit: `new_deaths`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = df_proc.corr()['new_deaths']\n",
    "print(\"Correlations to new_deaths:\")\n",
    "print(round(correlations.sort_values(ascending=False), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plus le coefficient de corrélation est proche des valeurs extrêmes -1 et 1, plus la corrélation linéaire entre les variables est forte. On ne souhaite donc garder que les colonnes ayant une corrélation absolue avec `new_deaths` supérieure à 0.7.\n",
    "\n",
    "![](https://i.imgur.com/VcJvg8V.png)\n",
    "\n",
    ">http://www.sthda.com/french/wiki/test-de-correlation-formule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On supprime les colonnes ayant une corrélation absolue inférieure à 0.7\n",
    "df_proc = df_proc.drop(df_proc.columns[abs(correlations) < 0.7], axis=1)\n",
    "\n",
    "# On rajoute la colonne date que l'on a supprimée dans l'analyse de forme, et qui n'était pas utile dans l'analyse de correlations.\n",
    "df_proc['date'] = df['date']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici donc les colonnes qui seront utilisées pour l'entraînement des modèles de machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_proc.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 Mise en forme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois les colonnes utiles sélectionnées, il est nécessaire d'effectuer une mise en forme des données disponibles, car un certain nombre de données ne sont pas représentées, ou sont négatives or c'est tout simplement impossible.\n",
    "\n",
    "On peut donc voir ci-après les différentes informations disponibles en fonction du temps qui seront utiles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df_proc.columns:\n",
    "  if column == 'date':\n",
    "    continue\n",
    "\n",
    "  fig = go.Figure([go.Scatter(x=df_proc['date'], y=df_proc[column])])\n",
    "  fig.update_layout(\n",
    "    title={\n",
    "      'text': column,\n",
    "      'xanchor': 'center',\n",
    "      'yanchor': 'top'\n",
    "    },\n",
    "    xaxis_title='date',\n",
    "    yaxis_title=column,\n",
    "  )\n",
    "  fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En observant attentivement le schéma du nombre de nouveaux morts chaque jour, il est possible d'apercevoir une saisonnalité, particularité que l'on ne retrouve que sur ce graphe. En effet, chaque week-end, le nombre de morts n'est pas forcément rempli, cela est flagrant entre octobre 2021 et janvier 2022: le nombre de morts chaque week-end est presque tout le temps nul. En revanche, j'en conclus que les données doivent être forcément reportées sur un jour suivant, par exemple le lundi ou le mardi, or ce n'est pas très visible sur ce graphe. \n",
    "\n",
    "TODO sure ?\n",
    "\n",
    "Cette saisonnalité est un réel problème dans cette base de données, car à conditions égales et en fonction du jour de la semaine, les données rapportées ne seront pas les mêmes. J'ai donc choisi de directement supprimer l'intégralité des week-ends de la base de données afin de ne pas induire le modèle de machine learning en erreur.\n",
    "\n",
    "D'autres approches auraient pu être: de rajouter le jour dans une nouvelle colonne de la base de données, mais cette solution n'est pas assez prise en compte par le modèle de machine learning. Ou encore de remplacer le nombre de morts du week-end par la moyenne pondérée des jours précédents et suivants, or le but est de prédire des données, il est n'est donc pas viable de n'utiliser seulement les données des jours précédents.\n",
    "\n",
    "TODO pas ouf comme explication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove week-ends\n",
    "df_proc['date'] = pd.to_datetime(df_proc['date'])\n",
    "df_proc = df_proc.drop(df_proc[df_proc['date'].dt.day_name() == 'Saturday'].index)\n",
    "df_proc = df_proc.drop(df_proc[df_proc['date'].dt.day_name() == 'Sunday'].index)\n",
    "\n",
    "# TODO Remove or use another way\n",
    "# Add weekday number to data\n",
    "# df_proc['date'] = pd.to_datetime(df_proc['date'])\n",
    "# df_proc['weekday'] = df_proc['date'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On veut maintenant pouvoir remplir les données qui ne sont pas représentées. Pour ce faire, j'ai choisi de récupérer la valeur la plus proche pour chaque donnée manquante et remplir la base donnée avec ces informations. Cela permet de ne pas avoir de saut de données tout en gardant un maximum de données réelles pour entrainer le modèle de machine learning.\n",
    "\n",
    "Une fois les données manquantes remplies, il faut supprimer les données négatives, car il est impossible qu'un nombre de morts soit négatif par exemple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill all the NaN values with nearest value\n",
    "df_proc = df_proc.fillna(method=\"bfill\").fillna(method=\"ffill\")\n",
    "print(f\"Any NaN remaining: {df_proc.isnull().sum().any()}\")\n",
    "\n",
    "# Remove rows with negative values because it's not possible\n",
    "df_proc = df_proc[df_proc.select_dtypes(include=[np.number]).ge(0).all(1)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df_proc.columns:\n",
    "  if column == 'date' or column == 'weekday':\n",
    "    continue\n",
    "\n",
    "  fig = go.Figure([go.Scatter(x=df_proc['date'], y=df_proc[column])])\n",
    "  fig.update_layout(\n",
    "    title={\n",
    "      'text': column,\n",
    "      'xanchor': 'center',\n",
    "      'yanchor': 'top'\n",
    "    },\n",
    "    xaxis_title='date',\n",
    "    yaxis_title=column,\n",
    "  )\n",
    "  fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le problème avec ce remplissage des données est que pour les colonnes `hosp_patients` et `icu_patients` ce remplissage n'est pas bien réalisé et ne peut pas être effectué simplement. Ces informations n'ont pas été récoltées depuis le début de la crise, et poseront problème au modèle. Pour palier à ce problème, j'ai choisir de supprimer l'intégralité des données récoltées avant le 30-03-2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_proc = df_proc.loc[(df_proc['date'] >= '2020-03-30')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df_proc.columns:\n",
    "  if column == 'date' or column == 'weekday':\n",
    "    continue\n",
    "\n",
    "  fig = go.Figure([go.Scatter(x=df_proc['date'], y=df_proc[column])])\n",
    "  fig.update_layout(\n",
    "    title={\n",
    "      'text': column,\n",
    "      'xanchor': 'center',\n",
    "      'yanchor': 'top'\n",
    "    },\n",
    "    xaxis_title='date',\n",
    "    yaxis_title=column,\n",
    "  )\n",
    "  fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme on peut le voir sur les graphes ci-dessus, une seule valeur est vraiment adhérente compte tenu des données adjacentes, il est donc nécessaire de la supprimer. Cependant, les données sont bien trop disparates, et il n'est donc pas possible de supprimer cette donnée automatiquement. La valeur que l'on retrouve le 09-02-2020 est donc retirée manuellement, permettant ainsi de lisser les données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On veut retirer l'outlier de new_deaths du 9 février 2021\n",
    "df_proc = df_proc[df_proc['date'] != '2021-02-09']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure([\n",
    "  go.Scatter(x=df_proc['date'], y=df_proc['new_deaths'])\n",
    "])\n",
    "fig.update_layout(\n",
    "  title={\n",
    "    'text': column,\n",
    "    'xanchor': 'center',\n",
    "    'yanchor': 'top'\n",
    "  },\n",
    "  xaxis_title='date',\n",
    "  yaxis_title=column,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Prédictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Définitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ces dernières années, les méthodes d'apprentissage automatique sont devenues omniprésentes dans la vie quotidienne. Des recommandations automatiques de films à regarder, de plats à commander ou de produits à acheter, la radio en ligne personnalisée ou la reconnaissance de vos amis sur vos photos, de nombreux sites Web et appareils modernes sont dotés d'algorithmes d'apprentissage automatique.\n",
    "\n",
    "L'apprentissage automatique, aussi appelé \"machine learning\", est une sous-catégorie de l'intelligence artificielle qui est la capacité d'une machine à imité le comportement humain. L'intelligence artificielle est utilisée pour effectuer des tâches complexes d'une manière similaire à la façon dont les humains résolvent les problèmes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.1 Types d'algorithmes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il existe deux types d'algorithmes d'apprentissage:\n",
    "- Supervisé: algorithmes qui automatisent le processus de décision en généralisant à partir d'exemples connus. Dans ce cas, l'utilisateur fournit à l'algorithme des paires d'entrées et de sorties souhaitées, et l'algorithme trouve un moyen de produire la sortie souhaitée à partir d'une entrée. En effet, l'algorithme est capable de créer une sortie pour une entrée qu'il n'a jamais vue auparavant sans aucune aide.\n",
    "- Non supervisés: dans ce type d'algorithmes, seules les données d'entrée sont connues, et aucune donnée de sortie connue n'est donnée à l'algorithme. \n",
    "\n",
    "Dans notre cas, comme les données à prédire sont connues, nous utiliserons des algorithmes d'apprentissage supervisé."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.2 Types de problèmes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce type d'algorithmes, il existe deux types de problèmes d'apprentissage supervisé:\n",
    "- Regression: l'objectif est de prédire un nombre continu. (Ex: prédire le prix d'une maison)\n",
    "- Classification: l'objectif est de prédire une classe, qui est un choix parmi une liste prédéfinie de possibilités. (Ex: prédire la race d'un animal)\n",
    "\n",
    "Dans notre cas, comme les données à prédire sont le nombre de nouveaux morts par jour, nous sommes dans un cas de régression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.3 Caractéristique d'un modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans l'apprentissage supervisé, nous voulons construire un modèle à partir de données, et être ensuite capables de faire des prédictions précises sur de nouvelles données. Si un modèle est capable de faire des prédictions précises sur des données inconnues, on dit qu'il est capable de généraliser de l'ensemble d'apprentissages à l'ensemble de tests. Nous voulons construire un modèle capable de généraliser aussi précisément que possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Régression avec données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que les données récoltées ont été traitées pour être utilisées au mieux, il va nous être possible de les utiliser afin d'entrainer un modèle de machine learning et de pouvoir prédire des données que nous ne connaissons pas encore. Dans notre cas, nous utiliserons l'apprentissage automatique afin de pouvoir prédire le nombre de nouveaux morts chaque jour à cause du Covid-19. \n",
    "\n",
    "Dans le cadre de ce projet, la temporalité des données est une informations très importantes, nous allons donc travailler autour de cette caractéristique en posisitionnant la colonne `date` comme index du tableau de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_used = df_proc\n",
    "df_used = df_used.set_index('date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Division des données:**\n",
    "\n",
    "Afin d'évaluer au mieux les performances d'un modèle de machine learning, il faut découper les données en trois grandes parties:\n",
    "- les données d'entrainement (`train set`)\n",
    "- les données de validation (`validation set`)\n",
    "- les données de test (`test set`)\n",
    "\n",
    "Pour commencer, il faut diviser en deux groupes les données de base: données de test, et données d'entrainement, en donnant un plus grande part aux données d'entrainement. Dans notre cas, il faut aussi prendre en compte la temporalité, il faut donc que les données d'entrainement soient au début, et les données de test à la fin. TODO 80%\n",
    "Une fois les données divisées en deux catégories, il faut mettre de côté les données de test, ce sont les données qui nous permettront d'évaluer les différents modèles sur des données qu'ils ne connaissent pas.\n",
    "\n",
    "<img src=\"https://i.imgur.com/JITVKcx.png\" height=\"300\">\n",
    "\n",
    "Ensuite, pour évaluer un modèle lors de son entrainement, il est nécessaire de récupérer une partie des données restantes (données d'entrainement) afni de former les données de validation. Afin d'éviter tout biais dans l'apprentissage du modèle, il est nécessaire d'utiliser un `K-Fold cross validation` permettant de faire varier les données d'entrainement et les données de validation lors de l'apprentissage. Ce `K-Fold cross validation` divise en K parties les données d'entrainement, et forme une partie avec les données de validation, afin de tester toutes les combinaisons possibles d'entrainement.\n",
    "\n",
    "<img src=\"https://i.imgur.com/VZrQkkD.png\" height=\"300\">\n",
    "\n",
    "Cependant, dans ce projet, la temporalité des évènements importe énormément, il n'est donc pas viable d'utiliser un `K-Fold cross validation`, il faut plutot utiliser les `TimeSeriesSplit`. Cette classe permet d'effectuer de la même manière qu'un K-Fold, une séparation en K parties des données d'entrainement. Cependant, pour chaque entrainement du modèle, on utilise les données des entrainements précédents, ainsi que les 'nouvelles' données d'entrainement, le tout en fonction du temps.\n",
    "\n",
    "<img src=\"https://i.imgur.com/ZDkrK1m.png\" height=\"300\">\n",
    "\n",
    "**Evaluation des modèles:**\n",
    "\n",
    "Après avoir entrainer les différents modèles de machine learning, il est nécessaire de les évaluer pour en déterminer les performances. Afin d'évaluer les différents modèles de machine learning, il faut déterminer une fonction d'évaluation qui sera utilisée par la suite pour évaluer tous les modèles. Pour ce faire, j'ai choisi la fonction `r2`, cette fonction représente le taux de corrélation des valeurs prédites avec les vraies données. Cette fonction d'évaluation a la particularité d'être normalisée, et d'être sous la forme d'un pourcentage, il est donc plus simple d'en interpréter les résultats. Cependant, il faut faire attention, car si les prédictions sont fortement anti-corrélées aux vraies données, le résultat sera aussi élevé.\n",
    "\n",
    "**Comparaison avec une ligne de base:**\n",
    "\n",
    "Pour voir si le modèle évalué est performant, il est nécessaire de le comparer avec une méthode dite 'naïve' de prédiction de données. En effet, si utiliser l'aléatoire est plus performant que de passer par le modèle de machine learning entrainé, c'est que le modèle n'est pas efficace. Comme dans ce projet, la temporalité des données est importante, j'ai choisi comme ligne de base de récupérer pour chaque jour le nombre de morts du jour précédent. Voici ci-dessous un exemple, avec les données représentés par des points orange, et la ligne de base décrite précédemment représentée par des traits orange.\n",
    "\n",
    "<img src=\"https://i.imgur.com/fFW9qbd.png\" height=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "X_train = df_used[:'2021-09'].drop(['new_deaths'], axis = 1)\n",
    "y_train = df_used.loc[:'2021-09', 'new_deaths']\n",
    "\n",
    "X_test = df_used['2021-10':].drop(['new_deaths'], axis = 1)\n",
    "y_test = df_used.loc['2021-10':, 'new_deaths']\n",
    "\n",
    "y_baseline = df_used.loc['2021-10':, 'new_deaths'].shift(periods=1) # On prend à chaque fois la valeur de la veille, seulement sur la partie prédiction\n",
    "y_baseline = y_baseline.fillna(0)\n",
    "\n",
    "cv = TimeSeriesSplit(n_splits=10)\n",
    "scoring = 'r2'\n",
    "njobs = -1 # On utilise autant de coeurs disponibles possibles sur le processeur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour chaque modèle de machine learning, les mêmes opérations vont être effectuées:\n",
    "- création d'un `pipeline`\n",
    "- définition des paramètres du modèle\n",
    "- utilisation d'un `GridSearch` pour combiner le tout\n",
    "- entrainement du modèle\n",
    "- évaluation du modèle\n",
    "- visualisation des résultats\n",
    "\n",
    "**Création d'un `Pipeline`:**\n",
    "\n",
    "TODO\n",
    "\n",
    "**Définition des paramètres du modèle:**\n",
    "\n",
    "Pour chaque modèle, il existe un certains nombre de paramètres pouvant être modifiés afin d'augmenter les performances du modèle en question. Comme l'ajustement de ces paramètres est une tâche très complexe, et dépend du cas que l'on souhaite traiter, j'ai utilisé une grille référencant les différents paramètres à modifier. Ainsi, toutes les combinaisons des paramètres seront testées afin d'en faire ressortir la meilleure par la méthode du `brut force`: test de toutes les combinaisons possibles.\n",
    "\n",
    "**Utilisation d'un `GridSearch` pour combiner le tout:**\n",
    "\n",
    "La classe `GridSearch` permet de combiner le `Pipeline` ainsi que la grille de paramètre, et d'autres fonctionnalités qui ne sont pas traitées dans le cadre de ce projet. Elle permet aussi de spécifier la fonction d'évaluation, la `cross validation`, et le nombre de coeurs de processeur utilisés. \n",
    "\n",
    "**Entrainement du modèle:**\n",
    "\n",
    "Une fois la configuration terminée, il faut lancer l'entrainement des modèles sur les données d'entrainement.\n",
    "\n",
    "**Evaluation du modèle:**\n",
    "\n",
    "Afin d'évaluer les modèles, il est nécessaire de récolter le score de la fonction `r2` du modèle sur les données d'entrainement, les données de test, ainsi que sur la ligne de base pour pouvoir les comparer par la suite.\n",
    "\n",
    "**Visualisation des résultats:**\n",
    "\n",
    "Enfin, la visualisation des différentes données permet de mieux représenter les résultats obtenus précédemment. En effet, pour chaque modèle, un graphe est construit représentant les données d'entrainement, les données de test, l'entrainement du modèle, la prédiction du modèle, ainsi que la ligne de base. Ces graphes sont interactifs: il est possible de ne retirer l'affichage de certaines données en cliquant sur la légende, de zoomer sur les axes en sélectionnant une zone, etc ... En survolant les données il est aussi possible de voir le détail de celles-ci."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.1 Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2.1.1 Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "pipeline = Pipeline([(\"scaler\", MinMaxScaler()), (\"gaussian\", GaussianNB())])\n",
    "\n",
    "param_grid = {}\n",
    "\n",
    "grid = GridSearchCV(pipeline, param_grid=param_grid, cv=cv, scoring=scoring, n_jobs=njobs) \n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters: {grid.best_params_}\")\n",
    "print(f\"Training set score: {grid.score(X_train, y_train):.2f}\")\n",
    "print(f\"Test set score: {grid.score(X_test, y_test):.2f}\")\n",
    "# regression_results(y_test.values, grid.predict(X_test), y_baseline)\n",
    "scatter_plot(X_train.index, X_test.index, X_train.index, X_test.index, y_train, y_test, grid.predict(X_train), grid.predict(X_test), y_baseline, \"Gaussian\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2.1.1 Multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "pipeline = Pipeline([(\"scaler\", MinMaxScaler()), (\"multinomial\", MultinomialNB())])\n",
    "\n",
    "param_grid = {\n",
    "\t'multinomial__alpha': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(pipeline, param_grid=param_grid, cv=cv, scoring=scoring, n_jobs=njobs)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters: {grid.best_params_}\")\n",
    "print(f\"Training set score: {grid.score(X_train, y_train):.2f}\")\n",
    "print(f\"Test set score: {grid.score(X_test, y_test):.2f}\")\n",
    "# regression_results(y_test.values, grid.predict(X_test), y_baseline)\n",
    "scatter_plot(X_train.index, X_test.index, X_train.index, X_test.index, y_train, y_test, grid.predict(X_train), grid.predict(X_test), y_baseline, \"Multinomial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.2 K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "pipeline = Pipeline([(\"scaler\", MinMaxScaler()), (\"knn\", KNeighborsRegressor())])\n",
    "\n",
    "param_grid = {\n",
    "\t'knn__n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 20, 30, 40]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(pipeline, param_grid=param_grid, cv=cv, scoring=scoring, n_jobs=njobs)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters: {grid.best_params_}\")\n",
    "print(f\"Training set score: {grid.score(X_train, y_train):.2f}\")\n",
    "print(f\"Test set score: {grid.score(X_test, y_test):.2f}\")\n",
    "# regression_results(y_test.values, grid.predict(X_test), y_baseline)\n",
    "scatter_plot(X_train.index, X_test.index, X_train.index, X_test.index, y_train, y_test, grid.predict(X_train), grid.predict(X_test), y_baseline, \"Knn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.3 Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2.3.1 Simple Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "pipeline = Pipeline([(\"scaler\", MinMaxScaler()), (\"tree\", DecisionTreeRegressor())])\n",
    "\n",
    "param_grid = {\n",
    "\t'tree__criterion': [\"squared_error\", \"absolute_error\", \"poisson\", \"friedman_mse\"], \n",
    "\t'tree__max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, None]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(pipeline, param_grid=param_grid, cv=cv, scoring=scoring, n_jobs=njobs)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters: {grid.best_params_}\")\n",
    "print(f\"Training set score: {grid.score(X_train, y_train):.2f}\")\n",
    "print(f\"Test set score: {grid.score(X_test, y_test):.2f}\")\n",
    "# regression_results(y_test.values, grid.predict(X_test), y_baseline)\n",
    "scatter_plot(X_train.index, X_test.index, X_train.index, X_test.index, y_train, y_test, grid.predict(X_train), grid.predict(X_test), y_baseline, \"Decision tree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2.3.2 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "pipeline = Pipeline([(\"scaler\", MinMaxScaler()), (\"forest\", RandomForestRegressor())])\n",
    "\n",
    "param_grid = {\n",
    "\t'forest__criterion': [\"squared_error\", \"absolute_error\", \"poisson\"], \n",
    "\t'forest__max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \n",
    "\t'forest__n_estimators': [1, 5, 10, 50, 100, 500, 1000]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(pipeline, param_grid=param_grid, cv=cv, scoring=scoring, n_jobs=njobs)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters: {grid.best_params_}\")\n",
    "print(f\"Training set score: {grid.score(X_train, y_train):.2f}\")\n",
    "print(f\"Test set score: {grid.score(X_test, y_test):.2f}\")\n",
    "# regression_results(y_test.values, grid.predict(X_test), y_baseline)\n",
    "scatter_plot(X_train.index, X_test.index, X_train.index, X_test.index, y_train, y_test, grid.predict(X_train), grid.predict(X_test), y_baseline, \"Random forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2.3.3 Gradient Boosted Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "pipeline = Pipeline([(\"scaler\", MinMaxScaler()), (\"gradient_boosting\", GradientBoostingRegressor())])\n",
    "\n",
    "param_grid = {\n",
    "\t'gradient_boosting__criterion': [\"squared_error\", \"friedman_mse\"], \n",
    "\t'gradient_boosting__loss': [\"squared_error\", \"absolute_error\", \"huber\", \"quantile\"], \n",
    "\t'gradient_boosting__n_estimators': [1, 5, 10, 50, 100, 500, 1000], \n",
    "\t'gradient_boosting__learning_rate': [0, 0.001, 0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(pipeline, param_grid=param_grid, cv=cv, scoring=scoring, n_jobs=njobs)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters: {grid.best_params_}\")\n",
    "print(f\"Training set score: {grid.score(X_train, y_train):.2f}\")\n",
    "print(f\"Test set score: {grid.score(X_test, y_test):.2f}\")\n",
    "# regression_results(y_test.values, grid.predict(X_test), y_baseline)\n",
    "scatter_plot(X_train.index, X_test.index, X_train.index, X_test.index, y_train, y_test, grid.predict(X_train), grid.predict(X_test), y_baseline, \"Gradient boosted trees\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.4 Support Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "pipeline = Pipeline([(\"scaler\", MinMaxScaler()), (\"svr\", SVR())])\n",
    "\n",
    "param_grid = {\n",
    "\t'svr__C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], \n",
    "\t'svr__gamma': [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(pipeline, param_grid=param_grid, cv=cv, scoring=scoring, n_jobs=njobs)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters: {grid.best_params_}\")\n",
    "print(f\"Training set score: {grid.score(X_train, y_train):.2f}\")\n",
    "print(f\"Test set score: {grid.score(X_test, y_test):.2f}\")\n",
    "# regression_results(y_test.values, grid.predict(X_test), y_baseline)\n",
    "scatter_plot(X_train.index, X_test.index, X_train.index, X_test.index, y_train, y_test, grid.predict(X_train), grid.predict(X_test), y_baseline, \"SVR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.5 Multi-Layer Perceptrons (Deep Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "pipeline = Pipeline([(\"scaler\", MinMaxScaler()), (\"mlp\", MLPRegressor())])\n",
    "\n",
    "param_grid = {\n",
    "\t'mlp__hidden_layer_sizes': [1, 10, 100], \n",
    "\t'mlp__alpha': [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10], \n",
    "\t'mlp__solver': ['lbfgs', 'sgd'],\n",
    "\t'mlp__max_iter': [10, 100]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(pipeline, param_grid=param_grid, cv=cv, scoring=scoring, n_jobs=njobs)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters: {grid.best_params_}\")\n",
    "print(f\"Training set score: {grid.score(X_train, y_train):.2f}\")\n",
    "print(f\"Test set score: {grid.score(X_test, y_test):.2f}\")\n",
    "# regression_results(y_test.values, grid.predict(X_test), y_baseline)\n",
    "scatter_plot(X_train.index, X_test.index, X_train.index, X_test.index, y_train, y_test, grid.predict(X_train), grid.predict(X_test), y_baseline, \"MLP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 Régression sans données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_used = df_proc\n",
    "df_used = df_used.set_index('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from datetime import timedelta\n",
    "\n",
    "days = 14\n",
    "\n",
    "# TODO lui donner aussi le nombre de morts des jours précédents, possible ?\n",
    "\n",
    "last_date = df_used.index[-1]\n",
    "new_date = df_used.index[-1] + timedelta(days=1)\n",
    "\n",
    "date_range = pd.date_range(new_date, periods=days, freq='D')\n",
    "days_to_predict = pd.DataFrame({'date': date_range}) \n",
    "days_to_predict = days_to_predict.set_index('date')\n",
    "\n",
    "df_pred = df_used.append(days_to_predict)\n",
    "\n",
    "X_train = df_pred.shift(periods=days)[:last_date].drop(['new_deaths'], axis = 1).dropna()\n",
    "y_train = df_pred.loc[df_pred.index[days]:last_date, 'new_deaths']\n",
    "\n",
    "X_pred = df_pred.shift(periods=days)[new_date:].drop(['new_deaths'], axis = 1)\n",
    "\n",
    "cv = TimeSeriesSplit(n_splits=10)\n",
    "scoring = 'r2'\n",
    "njobs = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "pipeline = Pipeline([(\"scaler\", MinMaxScaler()), (\"mlp\", MLPRegressor())])\n",
    "\n",
    "param_grid = {\n",
    "\t'mlp__hidden_layer_sizes': [1, 10, 100], \n",
    "\t'mlp__alpha': [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10], \n",
    "\t'mlp__solver': ['lbfgs', 'sgd'],\n",
    "\t'mlp__max_iter': [10, 100]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(pipeline, param_grid=param_grid, cv=cv, scoring=scoring, n_jobs=njobs)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters: {grid.best_params_}\")\n",
    "print(f\"Training set score: {grid.score(X_train, y_train):.2f}\")\n",
    "\n",
    "fig = go.Figure([\n",
    "    go.Scatter(x=X_train.index, y=y_train, name='data'),\n",
    "    go.Scatter(x=X_pred.index, y=grid.predict(X_pred), name='predictions')\n",
    "])\n",
    "\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'text': \"MLP forecasting\",\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'\n",
    "    },\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='New deaths'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knn pour prédire la forme des features, et ensuite utiliser ces features approximée pour approximer la feature à prédire\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Bibliographie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1 Sources documentaires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2 Sources illustratives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image KFold: https://www.researchgate.net/figure/The-technique-of-KFold-cross-validation-illustrated-here-for-the-case-K-4-involves_fig10_278826818"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e0e5403d142268d2ee298e33f5e7a604efb2100ea4d766ecddda8137e331615a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit (windows store)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
